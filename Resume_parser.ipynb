{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required installations\n",
    "\n",
    "#pip install fitz\n",
    "#pip install nltk\n",
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\meera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the name.\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME',[pattern],on_match=None)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "  \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the mobile number\n",
    "import re\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the email\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting skills\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\meera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Getting education\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', 'Bachelor of Engineering',\n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "                \n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecation: 'getText' removed from class 'Page' after v1.19.0 - use 'get_text'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading the resume file as pdf and converting it into text\n",
    "\n",
    "import sys, fitz\n",
    "\n",
    "fname = \"Yash_resume_1.pdf\"       # Enter the path of the resume here.\n",
    "doc = fitz.open(fname)\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text = text + str(page.getText())\n",
    "    \n",
    "tx =\" \".join(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all details:\n",
    "name = extract_name(tx)\n",
    "email = extract_email(tx)\n",
    "phone_no = extract_mobile_number(tx)\n",
    "education = extract_education(tx)\n",
    "skills = extract_skills(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('file.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps({\"Name\":name, \"Email\":email, \"Phone\":phone_no, \"Education\":education, \"Skills\":skills}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open('file.json','r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
